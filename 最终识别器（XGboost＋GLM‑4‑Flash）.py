# -*- coding: utf-8 -*-
"""
å·¥ä¸šçº§æ‰¹é‡è¯†åˆ«è„šæœ¬ï¼ˆä¿®å¤ç‰ˆï¼‰ï¼šXGBoost + GLM-4-Flash
åŠŸèƒ½ï¼š
1. ã€ä¿®å¤ã€‘æ–‡ä»¶è¯»å–å¢åŠ â€œæš´åŠ›å®¹é”™æ¨¡å¼â€ï¼Œè§£å†³ UnicodeDecodeError
2. è‡ªåŠ¨æ£€æµ‹ç¯å¢ƒå¹¶å®‰è£…
3. æ”¯æŒæ–­ç‚¹ç»­ä¼ 
"""

import sys
import subprocess
import os


# ==========================================
# 0. è‡ªåŠ¨å®‰è£…ç¼ºå¤±çš„åº“
# ==========================================
def auto_install_packages():
    requirements = {
        "zhipuai": "zhipuai",
        "pandas": "pandas",
        "xgboost": "xgboost",
        "sklearn": "scikit-learn",
        "tqdm": "tqdm",
        "openpyxl": "openpyxl",
        "scipy": "scipy"
    }
    print("-" * 30)
    print("æ­£åœ¨æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")
    for import_name, pip_name in requirements.items():
        try:
            __import__(import_name)
        except ImportError:
            try:
                subprocess.check_call([sys.executable, "-m", "pip", "install", pip_name, "-i",
                                       "https://pypi.tuna.tsinghua.edu.cn/simple"])
                print(f"âœ… {pip_name} å®‰è£…æˆåŠŸï¼")
            except Exception:
                pass
    print("âœ… ç¯å¢ƒæ£€æŸ¥å®Œæ¯•ã€‚")


auto_install_packages()

import csv
import time
import threading
import pandas as pd
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from scipy.sparse import hstack
from sklearn.feature_extraction.text import TfidfVectorizer
from xgboost import XGBClassifier
from zhipuai import ZhipuAI

# ==========================================
# 1. ç”¨æˆ·é…ç½®åŒº (è¯·ä¿®æ”¹è¿™é‡Œ)
# ==========================================

# ã€å¿…å¡«ã€‘ä½ çš„ GLM-4 API Key
API_KEY = "47c79b53c6de4a0ea193c0e7b3fa6393.WGMR1QBd8VoouIBM"

# è·¯å¾„é…ç½®
TRAIN_SAMPLE = r"D:\Desktop\sample_train_3000_glm.csv"
TRAIN_PRED = r"D:\Desktop\train_pred_sample_3000_glm.csv"

TARGET_FILE = r"D:\Desktop\å»é‡åœŸè€³å…¶åœ°éœ‡.csv"
CACHE_FILE = r"D:\Desktop\temp_glm_cache.csv"
FINAL_FILE = r"D:\Desktop\æœ€ç»ˆè¯†åˆ«ç»“æœ_27k.xlsx"

MAX_WORKERS = 5


# ==========================================
# 2. å¼ºåŠ›æ–‡ä»¶è¯»å–å‡½æ•° (æ ¸å¿ƒä¿®å¤)
# ==========================================

def robust_read_csv(file_path):
    """
    å°è¯•å¤šç§ç¼–ç è¯»å–æ–‡ä»¶ï¼Œå¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨'replace'æ¨¡å¼å¼ºåˆ¶è¯»å–
    """
    print(f"[Load] æ­£åœ¨å°è¯•è¯»å–æ–‡ä»¶: {file_path}")

    # æ–¹æ³•1: å°è¯• UTF-8-SIG (Excelå¸¸ç”¨)
    try:
        return pd.read_csv(file_path, encoding='utf-8-sig', dtype={'id': str})
    except UnicodeDecodeError:
        pass

    # æ–¹æ³•2: å°è¯• GB18030 (GBKçš„è¶…é›†ï¼Œæ”¯æŒæ›´å¤šå­—ç¬¦)
    try:
        return pd.read_csv(file_path, encoding='gb18030', dtype={'id': str})
    except UnicodeDecodeError:
        pass

    # æ–¹æ³•3: ç»æ€ - å¿½ç•¥é”™è¯¯å¼ºåˆ¶è¯»å– (encoding_errors='replace')
    print("[Warn] æ ‡å‡†ç¼–ç è¯»å–å¤±è´¥ï¼Œå¯ç”¨æš´åŠ›å®¹é”™æ¨¡å¼ï¼ˆéæ³•å­—ç¬¦å°†è¢«æ›¿æ¢ä¸º ?ï¼‰...")
    try:
        return pd.read_csv(file_path, encoding='utf-8', encoding_errors='replace', dtype={'id': str})
    except Exception as e:
        print(f"âŒ ä¸¥é‡é”™è¯¯: æ–‡ä»¶æ— æ³•è¯»å–ã€‚åŸå› : {e}")
        sys.exit(1)


# ==========================================
# 3. GLM-4 API è°ƒç”¨æ¨¡å—
# ==========================================

try:
    client = ZhipuAI(api_key=API_KEY)
except Exception:
    print("API Key é…ç½®é”™è¯¯æˆ– SDK åˆå§‹åŒ–å¤±è´¥")
    sys.exit(1)


def call_glm4_get_score(text, row_id):
    prompt_content = f"""
    è¯·åˆ†æä»¥ä¸‹æ¨æ–‡ï¼Œåˆ¤æ–­å…¶æ˜¯å¦ç”±æœºå™¨äºº/æ™ºèƒ½ä½“(Agent)ç”Ÿæˆã€‚
    æ¨æ–‡å†…å®¹ï¼š"{text}"

    è¯·åªè¾“å‡ºä¸€ä¸ª 0 åˆ° 1 ä¹‹é—´çš„æ¦‚ç‡æ•°å€¼ï¼Œè¡¨ç¤ºå®ƒæ˜¯æœºå™¨äººçš„å¯èƒ½æ€§ã€‚
    1.0 è¡¨ç¤ºè‚¯å®šæ˜¯æœºå™¨äººï¼Œ0.0 è¡¨ç¤ºè‚¯å®šæ˜¯äººç±»ã€‚
    ä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–æ–‡å­—ï¼Œåªè¾“å‡ºæ•°å­—ã€‚
    """

    for _ in range(3):
        try:
            response = client.chat.completions.create(
                model="glm-4-flash",
                messages=[{"role": "user", "content": prompt_content}],
                temperature=0.1,
                top_p=0.7
            )
            content = response.choices[0].message.content.strip()
            import re
            nums = re.findall(r"0\.\d+|1\.0|0|1", content)
            if nums:
                return float(nums[0])
            return 0.5
        except Exception:
            time.sleep(1)
    return 0.5


# ==========================================
# 4. æ‰¹é‡å¤„ç†é€»è¾‘
# ==========================================

def batch_process_glm(df):
    total = len(df)

    if os.path.exists(CACHE_FILE):
        print(f"[Resume] æ£€æµ‹åˆ°ç¼“å­˜æ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½è¿›åº¦...")
        try:
            df_cache = pd.read_csv(CACHE_FILE, dtype={'id': str})
            processed_map = dict(zip(df_cache['id'], df_cache['p_ai_glm']))
            print(f"[Resume] å·²å®Œæˆ {len(processed_map)} æ¡ã€‚")
        except:
            processed_map = {}
    else:
        processed_map = {}
        with open(CACHE_FILE, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['id', 'p_ai_glm'])

    pending_rows = []
    for idx, row in df.iterrows():
        rid = str(row['id'])
        if rid not in processed_map:
            pending_rows.append((rid, str(row['text'])))

    if not pending_rows:
        print("[Skip] æ‰€æœ‰æ•°æ®å‡å·²å¤„ç†å®Œæ¯•ï¼")
        df['p_ai'] = df['id'].astype(str).map(processed_map)
        return df

    print(f"[Start] å¼€å§‹å¤„ç† {len(pending_rows)} æ¡æ•°æ®...")
    file_lock = threading.Lock()

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_id = {
            executor.submit(call_glm4_get_score, text, rid): rid
            for rid, text in pending_rows
        }

        pbar = tqdm(total=len(pending_rows), desc="GLMè¯†åˆ«ä¸­")

        for future in as_completed(future_to_id):
            rid = future_to_id[future]
            try:
                score = future.result()
            except:
                score = 0.5

            with file_lock:
                with open(CACHE_FILE, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    writer.writerow([rid, score])

            processed_map[rid] = score
            pbar.update(1)

        pbar.close()

    df['p_ai'] = df['id'].astype(str).map(processed_map)
    df['p_ai'] = df['p_ai'].fillna(0.5)
    return df


# ==========================================
# 5. æ¨¡å‹è®­ç»ƒä¸èåˆæ¨¡å—
# ==========================================

def retrain_model_from_samples():
    print("-" * 30)
    print("[Train] æ­£åœ¨æ¢å¤ XGBoost æ¨¡å‹...")

    # åŒæ ·ä½¿ç”¨ robust_read_csv é˜²æ­¢æ ·æœ¬è¯»å–æŠ¥é”™
    df_train = robust_read_csv(TRAIN_SAMPLE)
    df_pred = robust_read_csv(TRAIN_PRED)

    texts = df_train['text'].fillna("").astype(str).tolist()
    labels = df_train['label'].astype(int).values
    glm_scores = df_pred['p_ai'].fillna(0.5).values.reshape(-1, 1)

    vec = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2)
    X_tfidf = vec.fit_transform(texts)

    X_final = hstack([X_tfidf, glm_scores])

    clf = XGBClassifier(
        n_estimators=400, max_depth=6, learning_rate=0.05,
        subsample=0.8, colsample_bytree=0.8, n_jobs=4
    )
    clf.fit(X_final, labels)

    return vec, clf


# ==========================================
# 6. ä¸»ç¨‹åº
# ==========================================

if __name__ == "__main__":
    if "ä½ çš„æ™ºè°±" in API_KEY:
        print("âŒ é”™è¯¯ï¼šè¯·å…ˆåœ¨ä»£ç ç¬¬ 59 è¡Œå¡«å…¥ä½ çš„ API Keyï¼")
        sys.exit(1)

    # 1. ä½¿ç”¨ä¿®å¤åçš„å‡½æ•°è¯»å–æ–‡ä»¶
    df_target = robust_read_csv(TARGET_FILE)
    print(f"âœ… æ–‡ä»¶è¯»å–æˆåŠŸï¼å…± {len(df_target)} æ¡æ•°æ®ã€‚")

    # 2. è¯†åˆ«æµç¨‹
    df_target = batch_process_glm(df_target)
    vectorizer, clf = retrain_model_from_samples()

    print("[Predict] æ­£åœ¨è¿›è¡Œæœ€ç»ˆæ¨ç†...")
    new_texts = df_target['text'].fillna("").astype(str).tolist()
    X_tfidf = vectorizer.transform(new_texts)
    X_glm = df_target['p_ai'].values.reshape(-1, 1)
    X_final = hstack([X_tfidf, X_glm])

    probs = clf.predict_proba(X_final)[:, 1]
    preds = (probs >= 0.7).astype(int)

    df_target['is_agent'] = preds
    df_target['agent_prob'] = probs

    print(f"[Save] ä¿å­˜ç»“æœåˆ°: {FINAL_FILE}")
    df_target.to_excel(FINAL_FILE, index=False)

    print("\n" + "=" * 30)
    print(f"ğŸ‰ æˆåŠŸï¼è¯†åˆ«å‡ºæ™ºèƒ½ä½“: {sum(preds)} ä¸ª")